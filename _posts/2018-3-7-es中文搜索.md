---
title: elasticsearch中文搜索
date: 2018-3-7 19:00:00
categories:
- elastic
tags:
- elastic
---

使用elasticsearch能够简单的实现搜索业务， 但是elasticsearch默认是不支持中文搜索的，我们需要安装中文分词插件来让其支持中文的搜索，用的最多的应该是`IKAnalyzer`中文分词了。
[ik的GitHub地址](https://github.com/medcl/elasticsearch-analysis-ik

# 安装

1. 查看es版本，使用`./elasticsearch --version`查看你的es版本(可以使用`whereis elasticsearch`来找es的可执行文件目录)
2. 去github上面找到对应的版本， es5.0之后的版本， ik的版本号是与之一一对应的， 如果你的版本在其中没有找到， 那么只能自己下载版本差异小的自己打包。
   比如我的es版本是`5.6.3`, ik并没有与之对应的版本， 但是有`5.6.8`,于是我就下载它
3. 下载好对应的版本之后， 如果并不能与es版本对应， 需要我们用maven进行手动打包。
    1. 查看maven是否安装 `whereis maven`没有安装的话使用`sudo apt-get install maven`（ubuntu）来简单的进行安装。
    2. 解压文件包 `tar -zxvf elasticsearch-analysis-ik-5.6.8.tar.gz`
    3. 进入解压后的文件夹， 然后`vi pom.xml`, 搜索`elasticsearch.version`将其中的版本改为`5.6.3`, 保存
    4. 运行命令`mvn package`, 等待执行完之后进行下一步
    5. `cd target/releases/`, 并解压生成的文件`unzip elasticsearch-analysis-ik-5.6.3.zip`
    6. 将解压后的文件夹拷贝至 `elasticsearch/plugin/`目录下， 重启elasticsearch
4. 验证是否生效 `curl -XGET 'http://localhost:9200/_analyze?pretty&analyzer=ik_max_word' -d '中华人民共和国'`, 如果能够正常分词说明安装成功了

# 使用

es 已经定义了多种常见的analyzer，用于分析我们存入的文档， 但有时候可能并不适合我们的项目， 于是我们需要自定义属于自己的analyzer。 analyzer在分析文档的时候分为三个步骤
`Character Filters` 负责将我们输入的文档进行字符串过滤， 例如去除html标签之类的，`Tokenizer`负责分词，将一段文本分成多个词条， `Token filter` 负责将词条进一步过滤， 比如将英文全部变为小写，抽取英文词干之类的操作， 之后就使用这些`token`（词条）创建倒排索引。（来附图）
![ik-1](/assets/images/postImages/elastic-ik-1.png)

简单的一个自定义analyzer

```json
{
  "settings": {
    "number_of_shards": 5,
    "number_of_replicas": 0,
    "analysis": {
      "analyzer": {
        "test": {
          "type": "custom",
          "tokenizer": "ik_max_word",
          "filter": [
            "stemmer"
          ],
          "chat_filter": []
        }
      }
    }
  }
}
```

上面的例子使用ik_max_word进行最大化分词， 然后使用stemmer进行抽取词干， 这个stemmer并不会对中文产生影响，这样就简单的定义了一个不区分中英文的analyzer， 在创建`mapping`的时候我们可以指定字段`"analyzer": "test"`使用这个analyzer。
ps: 虽然我们可以指定使用的`search_analyzer`但是并不建议这么做（默认与analyzer的值相同）， 因为查询和索引的analyzer不一致， 那么分析的结果也不一致，就会导致搜索不到正确的结果。

```shell
 curl -XGET 'http://localhost:9200/_analyze?pretty&tokenizer=ik_max_word&filter=stemmer' -d 'i have many dogs'
```

```json
{
  "tokens" : [
    {
      "token" : "i",
      "start_offset" : 0,
      "end_offset" : 1,
      "type" : "ENGLISH",
      "position" : 0
    },
    {
      "token" : "have",
      "start_offset" : 2,
      "end_offset" : 6,
      "type" : "ENGLISH",
      "position" : 1
    },
    {
      "token" : "mani",
      "start_offset" : 7,
      "end_offset" : 11,
      "type" : "ENGLISH",
      "position" : 2
    },
    {
      "token" : "dog",
      "start_offset" : 12,
      "end_offset" : 16,
      "type" : "ENGLISH",
      "position" : 3
    }
  ]
}
```

**可以使用chrome的es--head插件比较方便的测试。[点击安装](https://chrome.google.com/webstore/detail/elasticsearch-head/ffmkiejjmecolpfloofpjologoblkegm)**

后面可能写篇关于安装拼音分词的， 也可能不写毕竟网上文章已经很多了。。。。